{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46f02fda-a018-4316-8306-6c9833f09f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import dgl\n",
    "from dgl.nn.pytorch import edge_softmax\n",
    "import dgl.function as fn\n",
    "import dgl.data\n",
    "from dgl.nn.pytorch import GATConv\n",
    "\n",
    "def l0_train(logAlpha, beta=0.66, gamma=-0.1, zeta=1.1, eps=1e-20):\n",
    "    U = torch.rand(logAlpha.size()).type_as(logAlpha) + eps\n",
    "    s = torch.sigmoid((torch.log(U / (1 - U)) + logAlpha) / beta)\n",
    "    s_bar = s * (zeta - gamma) + gamma\n",
    "    mask = F.hardtanh(s_bar, 0, 1)\n",
    "    return mask\n",
    "\n",
    "def l0_test(logAlpha, beta=0.66, gamma=-0.1, zeta=1.1):\n",
    "    s = torch.sigmoid(logAlpha / beta)\n",
    "    s_bar = s * (zeta - gamma) + gamma\n",
    "    mask = F.hardtanh(s_bar, 0, 1)\n",
    "    return mask\n",
    "\n",
    "# Merged GAT Layer\n",
    "class MergedGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_heads, dropout, alpha, bias_l0, residual=False):\n",
    "        super(MergedGATLayer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.leaky_relu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        self.residual = residual\n",
    "        if self.residual and in_dim != (out_dim * num_heads):\n",
    "            self.res_fc = nn.Linear(in_dim, out_dim * num_heads, bias=False)\n",
    "        else:\n",
    "            self.res_fc = None\n",
    "\n",
    "        self.lin_l = nn.Linear(in_dim, num_heads * out_dim, bias=False)\n",
    "        self.lin_r = nn.Linear(in_dim, num_heads * out_dim, bias=False)\n",
    "        \n",
    "        self.att = nn.Parameter(torch.Tensor(1, num_heads, out_dim))\n",
    "        self.bias_l0 = nn.Parameter(torch.FloatTensor([bias_l0]))\n",
    "        self.beta = 0.66\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_normal_(self.lin_l.weight)\n",
    "        nn.init.xavier_normal_(self.lin_r.weight)\n",
    "        nn.init.xavier_normal_(self.att)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        if inputs.dim() != 2:\n",
    "            raise ValueError(\"Expected 2D input tensor, but got {}D\".format(inputs.dim()))\n",
    "\n",
    "        num_nodes = inputs.shape[0]\n",
    "        new_feature_size = self.out_dim * self.num_heads  # Assuming out_dim is defined in __init__\n",
    "\n",
    "        h_l = F.dropout(self.lin_l(inputs), p=self.dropout, training=self.training)\n",
    "        h_r = F.dropout(self.lin_r(inputs), p=self.dropout, training=self.training)\n",
    "\n",
    "        # Ensure that the total size matches before reshaping\n",
    "        if h_l.numel() != num_nodes * new_feature_size or h_r.numel() != num_nodes * new_feature_size:\n",
    "            raise RuntimeError(\"Mismatch in total elements for reshaping.\")\n",
    "\n",
    "        h_l = h_l.view(num_nodes, self.num_heads, -1)\n",
    "        h_r = h_r.view(num_nodes, self.num_heads, -1)\n",
    "\n",
    "        g.ndata['h_l'] = h_l\n",
    "        g.ndata['h_r'] = h_r\n",
    "\n",
    "        # Compute attention scores\n",
    "        g.apply_edges(self.edge_attention)\n",
    "        \n",
    "        # Apply edge softmax to normalize attention scores\n",
    "        g.edata['a'] = edge_softmax(g, g.edata['a'])\n",
    "\n",
    "        g.update_all(fn.u_mul_e('h_r', 'a', 'm'), fn.sum('m', 'h'))\n",
    "        h = g.ndata.pop('h').view(inputs.shape[0], -1)\n",
    "\n",
    "        # Apply residual connection\n",
    "        if self.residual:\n",
    "            if self.res_fc is not None:\n",
    "                res_out = self.res_fc(inputs)\n",
    "            else:\n",
    "                res_out = inputs\n",
    "            h += res_out \n",
    "\n",
    "        return h      \n",
    "        return h\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # GATv2 dynamic attention mechanism\n",
    "        h_l = edges.src['h_l']\n",
    "        h_r = edges.dst['h_r']\n",
    "        e = self.leaky_relu(h_l + h_r)\n",
    "        alpha = (e * self.att).sum(dim=-1, keepdim=True)\n",
    "\n",
    "        alpha = alpha + self.bias_l0\n",
    "        if self.training:\n",
    "            alpha = l0_train(alpha, beta=self.beta)\n",
    "        else:\n",
    "            alpha = l0_test(alpha, beta=self.beta)\n",
    "\n",
    "        return {'a': alpha}\n",
    "\n",
    "\n",
    "class MergedGAT(nn.Module):\n",
    "    def __init__(self, g, in_dim, num_hidden, num_classes, num_heads, dropout, alpha, bias_l0):\n",
    "        super(MergedGAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        self.graph = g\n",
    "\n",
    "  \n",
    "        self.layers = nn.ModuleList([\n",
    "            MergedGATLayer(in_dim, num_hidden, num_heads, dropout, alpha, bias_l0, residual=True)\n",
    "        ])\n",
    "\n",
    "        # Intermediate layers\n",
    "        for _ in range(3):\n",
    "            self.layers.append(MergedGATLayer(num_hidden * num_heads, num_hidden, num_heads, dropout, alpha, bias_l0, residual=True))\n",
    "\n",
    "        # Final layer\n",
    "        self.layers.append(MergedGATLayer(num_hidden * num_heads, num_classes, 1, dropout, alpha, bias_l0, residual=True))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = inputs\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = layer(self.graph, h)\n",
    "            if i < len(self.layers) - 1:\n",
    "                h = F.elu(h)\n",
    "                h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            else:\n",
    "                h = h.view(h.shape[0], -1)\n",
    "                h = h[:, :self.num_classes]\n",
    "        return h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c9dedc9-1675-46fb-95fb-39c750be6ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(g, num_classes):\n",
    "    # Hyperparameters\n",
    "    num_heads = 8\n",
    "    num_hidden = 256\n",
    "    dropout = 0.6\n",
    "    alpha = 0.2\n",
    "    bias_l0 = 0.1\n",
    "    in_dim = g.ndata['feat'].shape[1]\n",
    "    print(in_dim)\n",
    "    model = model = MergedGAT(g, in_dim=in_dim, num_hidden=256, num_classes=num_classes, num_heads=8, dropout=0.6, alpha=0.2, bias_l0=0.1)\n",
    "\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        logits = model(g.ndata['feat'])\n",
    "        loss = loss_func(logits[g.ndata['train_mask']], g.ndata['label'][g.ndata['train_mask']])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        _, train_indices = torch.max(logits, dim=1)\n",
    "        train_correct = torch.sum(train_indices[g.ndata['train_mask']] == g.ndata['label'][g.ndata['train_mask']])\n",
    "        train_accuracy = float(train_correct) / int(g.ndata['train_mask'].sum())\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(g.ndata['feat'])\n",
    "            val_loss = loss_func(val_logits[g.ndata['val_mask']], g.ndata['label'][g.ndata['val_mask']])\n",
    "            _, val_indices = torch.max(val_logits, dim=1)\n",
    "            val_correct = torch.sum(val_indices[g.ndata['val_mask']] == g.ndata['label'][g.ndata['val_mask']])\n",
    "            val_accuracy = float(val_correct) / int(g.ndata['val_mask'].sum())\n",
    "\n",
    "        print(f\"Epoch {epoch:05d} | Train Loss {loss.item():.4f} | Train Accuracy {train_accuracy:.4f} | Val Loss {val_loss.item():.4f} | Val Accuracy {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11aa2102-77c2-4c19-8968-6fbc5af847d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cora_data():\n",
    "    dataset = dgl.data.CoraGraphDataset()\n",
    "    g = dataset[0]\n",
    "    g = dgl.remove_self_loop(g)\n",
    "    g = dgl.add_self_loop(g)\n",
    "    degs = g.in_degrees().float()\n",
    "    norm = torch.pow(degs, -0.5)\n",
    "    norm[torch.isinf(norm)] = 0\n",
    "    g.ndata['norm'] = norm.unsqueeze(1)\n",
    "\n",
    "    return g, dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8160c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Amazon\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "\n",
    "def load_amazon_computers_data():\n",
    "    dataset = Amazon(root='/tmp/Amazon', name='Computers')\n",
    "    transform = RandomNodeSplit(split='random', num_train_per_class=20, num_val=500, num_test=1000)\n",
    "    data = transform(dataset[0])\n",
    "    g = dgl.from_networkx(to_networkx(data))\n",
    "    g.ndata['feat'] = data.x\n",
    "    g.ndata['label'] = data.y\n",
    "    g.ndata['train_mask'] = data.train_mask\n",
    "    g.ndata['val_mask'] = data.val_mask\n",
    "    g.ndata['test_mask'] = data.test_mask\n",
    "\n",
    "    return g, dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1de5c120-4cb6-4e78-891e-1b1f8f9f0559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (13752x767 and 1433x2048)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8212\\2840627552.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_amazon_computers_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8212\\885628936.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(g, num_classes)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feat'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8212\\2908960906.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8212\\2908960906.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, g, inputs)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mnew_feature_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_dim\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_heads\u001b[0m  \u001b[1;31m# Assuming out_dim is defined in __init__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mh_l\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin_l\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mh_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin_r\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (13752x767 and 1433x2048)"
     ]
    }
   ],
   "source": [
    "g, num_classes = load_amazon_computers_data()\n",
    "train_model(g, num_classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
