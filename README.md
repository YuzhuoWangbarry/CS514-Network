# CS514-Network

This research explores the optimization of Graph Attention Networks (GAT), specifically Graph Attention Network v2 (GATv2), using advanced techniques like residual connections, feature scaling, and hyper-parameter tuning. Focusing on the Amazon Co-purchase dataset, we demonstrate how these optimizations enhance GATv2's performance in node classification tasks. Additionally, we introduce Sparse Graph Attention Networks (SGAT), which employ a sparse attention mechanism for improved efficiency in large-scale graph datasets. Our comparative analysis of GCN and GAT models, along with the implementation of SGAT, provides insights into the evolving landscape of Graph Neural Networks and their practical applications in complex graph structures. The findings underscore the potential of GATv2 and SGAT in handling intricate graph-based data, marking a significant advancement in the field of graph neural network research.
